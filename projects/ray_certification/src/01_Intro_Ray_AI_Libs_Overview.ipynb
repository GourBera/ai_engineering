{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model\n",
    "\n",
    "¬© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª **Launch Locally**: You can run this notebook locally, but performance will be reduced.\n",
    "\n",
    "üöÄ **Launch on Cloud**: A Ray Cluster with 4 GPUs (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a quick end-to-end example to get a sense of what the Ray AI Libraries can do.\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li>Overview of the Ray AI Libraries</li>\n",
    "    <li>Quick end-to-end example</li>\n",
    "    <ul>\n",
    "      <li>Vanilla XGBoost code</li>\n",
    "      <li>Hyperparameter tuning with Ray Tune</li>\n",
    "      <li>Distributed training with Ray Train</li>\n",
    "      <li>Serving an ensemble model with Ray Serve</li>\n",
    "      <li>Batch inference with Ray Data</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[?25l\u001B[K\u001B[34m‚†ã\u001B[0m JSON API formula.jws.json                          Downloading  32.0MB/-------\n",
      "\u001B[K\u001B[34m‚†ã\u001B[0m JSON API cask.jws.json                             Downloading  15.3MB/-------\u001B[1F\u001B[K\u001B[34m‚†ã\u001B[0m JSON API formula.jws.json                          Downloading  32.0MB/-------\n",
      "\u001B[K\u001B[34m‚†ã\u001B[0m JSON API cask.jws.json                             Downloading  15.3MB/-------\u001B[1F\u001B[K\u001B[34m‚†ô\u001B[0m JSON API formula.jws.json                          Downloading  32.0MB/-------\n",
      "\u001B[K\u001B[34m‚†ô\u001B[0m JSON API cask.jws.json                             Downloading  15.3MB/-------\u001B[1F\u001B[K\u001B[34m‚†ô\u001B[0m JSON API formula.jws.json                          Downloading  32.0MB/-------\n",
      "\u001B[K\u001B[34m‚†ô\u001B[0m JSON API cask.jws.json                             Downloading  15.3MB/-------\u001B[1F\u001B[K\u001B[34m‚†ö\u001B[0m JSON API formula.jws.json                          Downloading  32.0MB/-------\n",
      "\u001B[K\u001B[34m‚†ö\u001B[0m JSON API cask.jws.json                             Downloading  15.3MB/-------\u001B[1F\u001B[K\u001B[34m‚†û\u001B[0m JSON API formula.jws.json                          Downloading  32.0MB/-------\n",
      "\u001B[K\u001B[32m‚úîÔ∏é\u001B[0m JSON API cask.jws.json                             Downloaded   15.3MB/ 15.3MB\u001B[1F\u001B[K\u001B[32m‚úîÔ∏é\u001B[0m JSON API formula.jws.json                          Downloaded   32.0MB/ 32.0MB\n",
      "\u001B[K\u001B[32m‚úîÔ∏é\u001B[0m JSON API cask.jws.json                             Downloaded   15.3MB/ 15.3MB\n",
      "\u001B[?25h\u001B[32m==>\u001B[0m \u001B[1mFetching downloads for: \u001B[32mlibomp\u001B[39m\u001B[0m\n",
      "\u001B[?25l\u001B[K\u001B[34m‚†ã\u001B[0m Bottle Manifest libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ã\u001B[0m Bottle Manifest libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ô\u001B[0m Bottle Manifest libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ô\u001B[0m Bottle Manifest libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ö\u001B[0m Bottle Manifest libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ö\u001B[0m Bottle Manifest libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†û\u001B[0m Bottle Manifest libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†û\u001B[0m Bottle Manifest libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ñ\u001B[0m Bottle Manifest libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ñ\u001B[0m Bottle Manifest libomp (21.1.8)                    Downloading  11.9KB/-------\u001B[0G\u001B[K\u001B[32m‚úîÔ∏é\u001B[0m Bottle Manifest libomp (21.1.8)                    Downloaded   11.9KB/ 11.9KB\n",
      "\u001B[?25h\u001B[?25l\u001B[K\u001B[34m‚†¶\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†¶\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†¥\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†¥\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†≤\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†≤\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†≥\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†≥\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ì\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ì\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ã\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ã\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ô\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ô\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ö\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†ö\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†û\u001B[0m Bottle libomp (21.1.8)\u001B[0G\u001B[K\u001B[34m‚†û\u001B[0m Bottle libomp (21.1.8) ##                          Downloading  49.2KB/586.2KB\u001B[0G\u001B[K\u001B[34m‚†ñ\u001B[0m Bottle libomp (21.1.8) ####                        Downloading  94.2KB/586.2KB\u001B[0G\u001B[K\u001B[34m‚†ñ\u001B[0m Bottle libomp (21.1.8) #############               Downloading 274.4KB/586.2KB\u001B[0G\u001B[K\u001B[34m‚†¶\u001B[0m Bottle libomp (21.1.8) #########################   Downloading 536.6KB/586.2KB\u001B[0G\u001B[K\u001B[32m‚úîÔ∏é\u001B[0m Bottle libomp (21.1.8)                             Downloaded  586.2KB/586.2KB\n",
      "\u001B[?25h\u001B[34m==>\u001B[0m \u001B[1mPouring libomp--21.1.8.arm64_tahoe.bottle.tar.gz\u001B[0m\n",
      "\u001B[34m==>\u001B[0m \u001B[1mCaveats\u001B[0m\n",
      "libomp is keg-only, which means it was not symlinked into /opt/homebrew,\n",
      "because it can override GCC headers and result in broken builds.\n",
      "\n",
      "For compilers to find libomp you may need to set:\n",
      "  export LDFLAGS=\"-L/opt/homebrew/opt/libomp/lib\"\n",
      "  export CPPFLAGS=\"-I/opt/homebrew/opt/libomp/include\"\n",
      "\u001B[34m==>\u001B[0m \u001B[1mSummary\u001B[0m\n",
      "üç∫  /opt/homebrew/Cellar/libomp/21.1.8: 9 files, 1.8MB\n",
      "\u001B[34m==>\u001B[0m \u001B[1mRunning `brew cleanup libomp`...\u001B[0m\n",
      "Disable this behaviour by setting `HOMEBREW_NO_INSTALL_CLEANUP=1`.\n",
      "Hide these hints with `HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n"
     ]
    }
   ],
   "source": [
    "# (Optional): If you get an XGBoostError at import, you might have to `brew install libomp` before importing xgboost again\n",
    "!brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourbera/Developer/projects/ray_certification/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-17 17:50:28,197\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2026-01-17 17:50:28,273\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2026-01-17 17:50:28,312\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import fastapi\n",
    "import pandas as pd\n",
    "import requests\n",
    "# macos: If you get an XGBoostError at import, you might have to `brew install libomp` before importing xgboost again\n",
    "import xgboost\n",
    "from pydantic import BaseModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ray\n",
    "import ray.tune\n",
    "import ray.train\n",
    "from ray.train.xgboost import XGBoostTrainer as RayTrainXGBoostTrainer\n",
    "from ray.train import RunConfig\n",
    "import ray.data\n",
    "import ray.serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of the Ray AI Libraries\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_AI_Libraries/Ray+AI+Libraries.png\" width=\"700px\" loading=\"lazy\">\n",
    "\n",
    "Built on top of Ray Core, the Ray AI Libraries inherit all the performance and scalability benefits offered by Core while providing a convenient abstraction layer for machine learning. These Python-first native libraries allow ML practitioners to distribute individual workloads, end-to-end applications, and build custom use cases in a unified framework.\n",
    "\n",
    "The Ray AI Libraries bring together an ever-growing ecosystem of integrations with popular machine learning frameworks to create a common interface for development.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=\"100%\" loading=\"lazy\">|\n",
    "|:-:|\n",
    "|Ray AI Libraries enable end-to-end ML development and provides multiple options for integrating with other tools and libraries from the MLOps ecosystem.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick end-to-end example\n",
    "\n",
    "For this classification task, you will apply a simple [XGBoost](https://xgboost.readthedocs.io/en/stable/) (a gradient boosted trees framework) model to the June 2021 [New York City Taxi & Limousine Commission's Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). \n",
    "\n",
    "The full dataset contains millions of samples of yellow cab rides, and the goal is to predict the tip amount.\n",
    "\n",
    "**Dataset features**\n",
    "* **`passenger_count`**\n",
    "    * Float (whole number) representing number of passengers.\n",
    "* **`trip_distance`** \n",
    "    * Float representing trip distance in miles.\n",
    "* **`fare_amount`**\n",
    "    * Float representing total price including tax, tip, fees, etc.\n",
    "* **`tolls_amount`**\n",
    "    * Float representing the total paid on tolls if any.\n",
    "\n",
    "**Target**\n",
    "* **`trip_amount`**\n",
    "    * Float representing the total paid as tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vanilla XGboost code\n",
    "\n",
    "Let's start with the vanilla XGBoost code to predict the tip amount for a NYC taxi cab data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"passenger_count\", \n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"tolls_amount\",\n",
    "]\n",
    "\n",
    "label_column = \"tip_amount\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to load the data and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourbera/Developer/projects/ray_certification/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to /Users/gourbera/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/2.archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.78G/1.78G [04:28<00:00, 7.14MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/gourbera/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/versions/2\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"elemento/nyc-yellow-taxi-trip-data\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # path = \"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_2021-03.parquet\"\n",
    "    path = \"/Users/gourbera/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/versions/2\"\n",
    "    df = pd.read_csv(path, usecols=features + [label_column])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features], df[label_column], test_size=0.2, random_state=42\n",
    "    )\n",
    "    dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to run `xgboost.train` given some hyperparameter dictionary `params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": "storage_folder = \"/Users/gourbera/ai_engineering/projects/ray_certification/storage_folder/\" # Modify this path to your local folder if it runs on your local environment"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_path = Path(storage_folder) / \"model.ubj\"\n",
    "\n",
    "def my_xgboost_func(params):    \n",
    "    evals_result = {}\n",
    "    dtrain, dtest = load_data()\n",
    "    bst = xgboost.train(\n",
    "        params, \n",
    "        dtrain, \n",
    "        num_boost_round=10, \n",
    "        evals=[(dtest, \"eval\")], \n",
    "        evals_result=evals_result,\n",
    "    )\n",
    "    # Use Path\n",
    "    bst.save_model(model_path)\n",
    "    print(f\"{evals_result['eval']}\")\n",
    "    return {\"eval-rmse\": evals_result[\"eval\"][\"rmse\"][-1]}\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"max_depth\": 5,\n",
    "    \"eta\": 0.1,\n",
    "}\n",
    "# my_xgboost_func(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyperparameter tuning with Ray Tune\n",
    "\n",
    "Let's use Ray Tune to run distributed hyperparameter tuning for the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = ray.tune.Tuner(  # Create a tuner\n",
    "#     my_xgboost_func,  # Pass it the training function which Ray Tune calls Trainable.\n",
    "#     param_space={  # Pass it the parameter space to search over\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"eval_metric\": \"rmse\",\n",
    "#         \"tree_method\": \"hist\",\n",
    "#         \"max_depth\": 6,\n",
    "#         \"eta\": ray.tune.uniform(0.01, 0.3),\n",
    "#     },\n",
    "#     run_config=RunConfig(storage_path=storage_folder),\n",
    "#     tune_config=ray.tune.TuneConfig(  # Tell it which metric to tune\n",
    "#         metric=\"eval-rmse\",\n",
    "#         mode=\"min\",\n",
    "#         num_samples=10,\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# results = tuner.fit()  # Run the tuning job\n",
    "# print(results.get_best_result().config)  # Get back the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that shows what Tune does:\n",
    "\n",
    "It is effectively scheduling many trials and returning the best performing one.\n",
    "\n",
    "<img src=\"https://bair.berkeley.edu/static/blog/tune/tune-arch-simple.png\" width=\"700px\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Distributed training with Ray Train\n",
    "\n",
    "In case your training data is too large, your training might take a long time to complete.\n",
    "\n",
    "To speed it up, shard the dataset across training workers and perform distributed XGBoost training.\n",
    "\n",
    "Let's redefine `load_data` to now load a different slice of the data given the worker index/rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # find out which training worker is running this code\n",
    "    train_ctx = ray.train.get_context()\n",
    "    worker_rank = train_ctx.get_world_rank()\n",
    "    print(f\"Loading data for worker {worker_rank}...\")\n",
    "\n",
    "    # build path based on training worker rank\n",
    "    month = \"01\"\n",
    "    year = 2015 + (worker_rank + 1) // 12\n",
    "    path = f\"/Users/gourbera/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/versions/2/yellow_tripdata_{year}-{month:02}.csv\"\n",
    "\n",
    "    # same as before\n",
    "    df = pd.read_csv(path, usecols=features + [label_column])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features], df[label_column], test_size=0.2, random_state=42\n",
    "    )\n",
    "    dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = f\"/Users/gourbera/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/versions/2/yellow_tripdata_2016-01.csv\"\n",
    "\n",
    "#     # same as before\n",
    "# df = pd.read_csv(path, usecols=features + [label_column])\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(\n",
    "# #         df[features], df[label_column], test_size=0.2, random_state=42\n",
    "# #     )\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run distributed XGBoost training using Ray Train's XGBoostTrainer - similar trainers exist for other popular ML frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(TrainController pid=28265)\u001B[0m Attempting to start training worker group of size 2 with the following resources: [{'CPU': 1}] * 2\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m Started training worker group of size 2: \n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m - (ip=127.0.0.1, pid=28274) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m - (ip=127.0.0.1, pid=28275) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001B[36m(RayTrainWorker pid=28274)\u001B[0m [17:57:04] Task [xgboost.ray-rank=00000000]:4f170b056f1c58a45964d48701000000 got rank 0\n",
      "\u001B[36m(RayTrainWorker pid=28274)\u001B[0m Loading data for worker 0...\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m [17:57:13] [0]\teval-rmse:4.69454\n",
      "\u001B[36m(RayTrainWorker pid=28275)\u001B[0m [17:57:04] Task [xgboost.ray-rank=00000001]:0d508ef904ac26ec3927855101000000 got rank 1\n",
      "\u001B[36m(RayTrainWorker pid=28275)\u001B[0m Loading data for worker 1...\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m [17:57:13] [1]\teval-rmse:7.99348\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m [17:57:13] [2]\teval-rmse:11.13523\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m [17:57:14] [3]\teval-rmse:14.00890\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m [17:57:14] [4]\teval-rmse:16.61324\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m [17:57:14] [5]\teval-rmse:18.96577\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m [17:57:14] [6]\teval-rmse:21.08774\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m [17:57:15] [7]\teval-rmse:23.00031\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m [17:57:15] [8]\teval-rmse:24.72340\n",
      "\u001B[36m(TrainController pid=28265)\u001B[0m [17:57:15] [9]\teval-rmse:26.27535\n",
      "\u001B[36m(RayTrainWorker pid=28274)\u001B[0m OrderedDict({'rmse': [np.float64(4.694536026540097), np.float64(7.993481356654193), np.float64(11.135227426445839), np.float64(14.008903036914825), np.float64(16.613237576742677), np.float64(18.965766421288833), np.float64(21.087742953321424), np.float64(23.00031419567475), np.float64(24.7234016559002), np.float64(26.275347656168254)]})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(metrics=None, checkpoint=None, error=None, path='/Users/gourbera/ray_results/ray_train_run-2026-01-17_17-57-01', metrics_dataframe=None, best_checkpoints=[], _storage_filesystem=<pyarrow._fs.LocalFileSystem object at 0x128f069f0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = RayTrainXGBoostTrainer(  # Create a trainer\n",
    "    my_xgboost_func,  # Pass it the training function\n",
    "    scaling_config=ray.train.ScalingConfig(\n",
    "        num_workers=2, use_gpu=False\n",
    "    ),  # Define how many training workers\n",
    "    train_loop_config=params,  # Pass it the hyperparameters\n",
    ")\n",
    "\n",
    "trainer.fit()  # Run the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that shows what Train does:\n",
    "\n",
    "A train controller will create training workers and execute the training function on each worker.\n",
    "\n",
    "Ray Train delegates the distributed training to the underlying XGBoost framework.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"700px\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Serving an ensemble model with Ray Serve\n",
    "\n",
    "Ray Serve allows for distributed serving of models and complex inference pipelines.\n",
    "\n",
    "Here is a diagram showing how to deploy an ensemble model with Ray Serve:\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/xjan103pcp94/3DJ7vVRxYIvcFO7JmIUMCx/77a45caa275ffa46f5135f4d6726dd4f/Figure_2_-_Fanout_and_ensemble.png\" width=\"700px\" loading=\"lazy\">\n",
    "\n",
    "Here is how the resulting code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING 2026-01-17 18:03:03,507 serve 26934 -- There are multiple deployments with the same name 'Model'. Renaming one to 'Model_1'.\n",
      "INFO 2026-01-17 18:03:03,514 serve 26934 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:03,616 controller 27185 -- Deploying new version of Deployment(name='Model', app='default') (initial target replicas: 1).\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:03,617 controller 27185 -- Deploying new version of Deployment(name='Model_1', app='default') (initial target replicas: 1).\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:03,618 controller 27185 -- Deploying new version of Deployment(name='Ensemble', app='default') (initial target replicas: 1).\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:03,723 controller 27185 -- Stopping 1 replicas of Deployment(name='Model', app='default') with outdated versions.\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:03,723 controller 27185 -- Adding 1 replica to Deployment(name='Model', app='default').\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:03,725 controller 27185 -- Stopping 1 replicas of Deployment(name='Model_1', app='default') with outdated versions.\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:03,725 controller 27185 -- Adding 1 replica to Deployment(name='Model_1', app='default').\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:03,727 controller 27185 -- Stopping 1 replicas of Deployment(name='Ensemble', app='default') with outdated versions.\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:03,727 controller 27185 -- Adding 1 replica to Deployment(name='Ensemble', app='default').\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:05,727 controller 27185 -- Replica(id='hgi6mc6s', deployment='Model', app='default') is stopped.\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:05,830 controller 27185 -- Replica(id='1v43e1pn', deployment='Model_1', app='default') is stopped.\n",
      "\u001B[36m(ServeController pid=27185)\u001B[0m INFO 2026-01-17 18:03:05,831 controller 27185 -- Replica(id='ktnnax4b', deployment='Ensemble', app='default') is stopped.\n",
      "INFO 2026-01-17 18:03:06,547 serve 26934 -- Application 'default' is ready at http://127.0.0.1:8000/ensemble.\n"
     ]
    }
   ],
   "source": [
    "app = fastapi.FastAPI()\n",
    "\n",
    "class Payload(BaseModel):\n",
    "    passenger_count: int\n",
    "    trip_distance: float\n",
    "    fare_amount: float\n",
    "    tolls_amount: float\n",
    "\n",
    "\n",
    "@ray.serve.deployment\n",
    "@ray.serve.ingress(app)\n",
    "class Ensemble:\n",
    "    def __init__(self, model1, model2):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    @app.post(\"/predict\")\n",
    "    async def predict(self, data: Payload) -> dict:\n",
    "        model1_prediction, model2_prediction = await asyncio.gather(\n",
    "            self.model1.predict.remote([data.model_dump()]),\n",
    "            self.model2.predict.remote([data.model_dump()]),\n",
    "        )\n",
    "        # Extract scalar values from numpy arrays\n",
    "        pred1 = float(model1_prediction[0]) if hasattr(model1_prediction, '__len__') else float(model1_prediction)\n",
    "        pred2 = float(model2_prediction[0]) if hasattr(model2_prediction, '__len__') else float(model2_prediction)\n",
    "        out = {\"prediction\": (pred1 + pred2) / 2}\n",
    "        return out\n",
    "\n",
    "\n",
    "@ray.serve.deployment\n",
    "class Model:\n",
    "    def __init__(self, path: str):\n",
    "        self._model = xgboost.Booster()\n",
    "        self._model.load_model(path)\n",
    "\n",
    "    def predict(self, data: list[dict]) -> list[float]:\n",
    "        # Make prediction\n",
    "        dmatrix = xgboost.DMatrix(pd.DataFrame(data))\n",
    "        model_prediction = self._model.predict(dmatrix)\n",
    "        return model_prediction\n",
    "\n",
    "\n",
    "# Run the deployment\n",
    "handle = ray.serve.run(\n",
    "    Ensemble.bind(\n",
    "        model1=Model.bind(model_path),\n",
    "        model2=Model.bind(model_path),\n",
    "    ),\n",
    "    route_prefix=\"/ensemble\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an HTTP request to the Ray Serve instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: {'prediction': 1.51448655128479}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(ServeReplica:default:Model_1 pid=29412)\u001B[0m /Users/gourbera/Developer/projects/ray_certification/.venv/lib/python3.13/site-packages/ray/serve/_private/replica.py:1634: UserWarning: Calling sync method 'predict' directly on the asyncio loop. In a future version, sync methods will be run in a threadpool by default. Ensure your sync methods are thread safe or keep the existing behavior by making them `async def`. Opt into the new behavior by setting RAY_SERVE_RUN_SYNC_IN_THREADPOOL=1.\n",
      "\u001B[36m(ServeReplica:default:Model_1 pid=29412)\u001B[0m   warnings.warn(\n",
      "\u001B[36m(ServeReplica:default:Model_1 pid=29412)\u001B[0m INFO 2026-01-17 18:03:14,546 default_Model_1 3325qrcd 779e6cc5-5c57-4f7d-b595-0209dc18ecd3 -- CALL predict OK 3.0ms\n",
      "\u001B[36m(ServeReplica:default:Model pid=29411)\u001B[0m INFO 2026-01-17 18:03:14,546 default_Model yb75o62z 779e6cc5-5c57-4f7d-b595-0209dc18ecd3 -- CALL predict OK 4.0ms\n",
      "\u001B[36m(ServeReplica:default:Ensemble pid=29413)\u001B[0m INFO 2026-01-17 18:03:14,522 default_Ensemble c24thshy 779e6cc5-5c57-4f7d-b595-0209dc18ecd3 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x11b722e40>.\n",
      "\u001B[36m(ServeReplica:default:Ensemble pid=29413)\u001B[0m INFO 2026-01-17 18:03:14,547 default_Ensemble c24thshy 779e6cc5-5c57-4f7d-b595-0209dc18ecd3 -- POST /ensemble/predict 200 40.1ms\n",
      "\u001B[36m(ServeReplica:default:Ensemble pid=29413)\u001B[0m INFO 2026-01-17 18:03:37,443 default_Ensemble c24thshy 052c5c75-c934-48da-85f0-65fab586a400 -- GET /ensemble 404 0.7ms\n",
      "\u001B[36m(ServeReplica:default:Model pid=29411)\u001B[0m /Users/gourbera/Developer/projects/ray_certification/.venv/lib/python3.13/site-packages/ray/serve/_private/replica.py:1634: UserWarning: Calling sync method 'predict' directly on the asyncio loop. In a future version, sync methods will be run in a threadpool by default. Ensure your sync methods are thread safe or keep the existing behavior by making them `async def`. Opt into the new behavior by setting RAY_SERVE_RUN_SYNC_IN_THREADPOOL=1.\n",
      "\u001B[36m(ServeReplica:default:Model pid=29411)\u001B[0m   warnings.warn(\n",
      "\u001B[36m(ServeReplica:default:Ensemble pid=29413)\u001B[0m INFO 2026-01-17 18:04:06,085 default_Ensemble c24thshy 519c99a1-b02f-44fd-9f5e-fd8df35819fe -- GET /ensemble 405 0.9ms\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/ensemble/predict\",\n",
    "        json={\n",
    "            \"passenger_count\": 1,\n",
    "            \"trip_distance\": 2.5,\n",
    "            \"fare_amount\": 10.0,\n",
    "            \"tolls_amount\": 0.5,\n",
    "        },\n",
    "        timeout=10\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    print(\"Prediction:\", response.json())\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Error: Could not connect to Ray Serve. Make sure the server is running with ray.serve.run()\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Batch inference with Ray Data\n",
    "\n",
    "Ray Data allows for distributed data processing through streaming execution across a heterogeneous cluster of CPUs and GPUs.\n",
    "\n",
    "This makes Ray Data ideal for workloads like compute-intensive data processing, data ingestion, and batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 17:58:51,192\tWARNING util.py:599 -- The argument ``concurrency`` is deprecated in Ray 2.51. Please specify argument ``compute`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n"
     ]
    }
   ],
   "source": [
    "class OfflinePredictor:\n",
    "    def __init__(self):\n",
    "        # Load expensive state\n",
    "        self._model = xgboost.Booster()\n",
    "        self._model.load_model(model_path)\n",
    "\n",
    "    def predict(self, data: list[dict]) -> list[float]:\n",
    "        # Make prediction in batch\n",
    "        dmatrix = xgboost.DMatrix(pd.DataFrame(data))\n",
    "        model_prediction = self._model.predict(dmatrix)\n",
    "        return model_prediction\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        batch[\"predictions\"] = self.predict(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Apply the predictor to the validation dataset\n",
    "prediction_pipeline = (\n",
    "    ray.data.read_csv(\n",
    "        \"/Users/gourbera/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/versions/2/yellow_tripdata_2015-01.csv\"\n",
    "    )\n",
    "    .select_columns(features)\n",
    "    .map_batches(OfflinePredictor, concurrency=(2, 10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the pipeline, we can execute it in a distributed manner by writing the output to a sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 17:58:53,978\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_4_0\n",
      "2026-01-17 17:58:54,014\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_4_0. Full logs are in /tmp/ray/session_2026-01-17_17-51-39_773213_26934/logs/ray-data\n",
      "2026-01-17 17:58:54,015\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_4_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV] -> ActorPoolMapOperator[Project->MapBatches(OfflinePredictor)] -> TaskPoolMapOperator[Write]\n",
      "2026-01-17 17:58:54,057\tINFO streaming_executor.py:686 -- [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n",
      "2026-01-17 17:58:54,058\tINFO progress_bar.py:155 -- Progress bar disabled because stdout is a non-interactive terminal.\n",
      "2026-01-17 17:58:54,059\tWARNING resource_manager.py:136 -- ‚ö†Ô∏è  Ray's object store is configured to use only 26.3% of available memory (2.0GiB out of 7.6GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2026-01-17 17:58:54,172\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadCSV->SplitBlocks(14)} ===\n",
      "2026-01-17 17:58:54,173\tINFO progress_bar.py:215 -- ReadCSV->SplitBlocks(14): Tasks: 1 [backpressured:tasks]; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 384.0MiB object store: Progress Completed 0 / ?\n",
      "2026-01-17 17:58:54,173\tINFO progress_bar.py:213 -- === Ray Data Progress {Project->MapBatches(OfflinePredictor)} ===\n",
      "2026-01-17 17:58:54,173\tINFO progress_bar.py:215 -- Project->MapBatches(OfflinePredictor): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2026-01-17 17:58:54,174\tINFO progress_bar.py:213 -- === Ray Data Progress {Write} ===\n",
      "2026-01-17 17:58:54,174\tINFO progress_bar.py:215 -- Write: Tasks: 0 [backpressured:tasks]; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 0 / ?\n",
      "2026-01-17 17:58:54,175\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2026-01-17 17:58:54,175\tINFO progress_bar.py:215 -- Running Dataset: dataset_4_0. Active & requested resources: 0/10 CPU, 0.0B/1.0GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n",
      "2026-01-17 17:58:59,200\tINFO progress_bar.py:215 -- ReadCSV->SplitBlocks(14): Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 18.3MiB object store: Progress Completed 3672330 / ?\n",
      "2026-01-17 17:58:59,202\tINFO progress_bar.py:215 -- Project->MapBatches(OfflinePredictor): Tasks: 1; Actors: 2; Queued blocks: 0 (0.0B); Resources: 2.0 CPU, 2.2MiB object store; [all objects local]: Progress Completed 3607902 / ?\n",
      "2026-01-17 17:58:59,202\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 56 / ?\n",
      "2026-01-17 17:58:59,203\tINFO progress_bar.py:215 -- Running Dataset: dataset_4_0. Active & requested resources: 3/10 CPU, 27.5MiB/1.0GiB object store: Progress Completed 56 / ?\n",
      "2026-01-17 17:59:04,285\tINFO progress_bar.py:215 -- ReadCSV->SplitBlocks(14): Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 18.3MiB object store: Progress Completed 9921971 / ?\n",
      "2026-01-17 17:59:04,285\tINFO progress_bar.py:215 -- Project->MapBatches(OfflinePredictor): Tasks: 0; Actors: 2; Queued blocks: 0 (0.0B); Resources: 2.0 CPU, 0.0B object store; [all objects local]: Progress Completed 9921971 / ?\n",
      "2026-01-17 17:59:04,285\tINFO progress_bar.py:215 -- Write: Tasks: 0; Actors: 0; Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store: Progress Completed 154 / ?\n",
      "2026-01-17 17:59:04,286\tINFO progress_bar.py:215 -- Running Dataset: dataset_4_0. Active & requested resources: 3/10 CPU, 18.3MiB/1.0GiB object store: Progress Completed 154 / ?\n",
      "2026-01-17 17:59:06,118\tINFO streaming_executor.py:304 -- ‚úîÔ∏è  Dataset dataset_4_0 execution finished in 12.10 seconds\n",
      "2026-01-17 17:59:06,162\tINFO dataset.py:5344 -- Data sink Parquet finished. 12748986 rows and 437.7MiB data written.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(ServeReplica:default:Ensemble pid=27194)\u001B[0m INFO 2026-01-17 17:59:29,412 default_Ensemble ktnnax4b 18d294db-a5f8-4c38-97d7-fda7b97aadf7 -- GET /ensemble 404 0.6ms\n",
      "\u001B[36m(ServeReplica:default:Ensemble pid=27194)\u001B[0m INFO 2026-01-17 17:59:35,189 default_Ensemble ktnnax4b a677aa52-9ab3-4ca8-9364-e87300c2fd74 -- GET /ensemble 404 2.2ms\n"
     ]
    }
   ],
   "source": [
    "prediction_pipeline.write_parquet(\"./xgboost_predictions\") #update this to your local path if runs on your local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the produced predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {storage_folder}/xgboost_predictions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm -rf {storage_folder}/xgboost_predictions/\n",
    "!rm {model_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
